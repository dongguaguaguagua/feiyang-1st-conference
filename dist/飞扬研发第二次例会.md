[toc]

## Github使用指南

## 爬虫详解

爬虫是什么呢？简单概括：自动从网络上收集信息的一种程序。复杂点来说，就是一整套关于数据请求、处理、存储的程序，这之间又涉及到关于网络、数据结构的一些知识。之后的教程也会分别详细介绍关于数据的采集、处理、存储这三方面的知识。

**爬虫是搜索引擎的一部分**。

你有没有想过为什么我们能够从百度上搜索到形形色色的网站。为什么百度会存有那些网站的信息，让你一搜就出来了。百度搜索的一部分工作，就是运行它自己的爬虫，从上千万的网站，采集到网页，然后存起来，等待你的搜索。

**爬虫违法吗？**

有关互联网法规正在逐步建立和完善中，就目前而言，如果抓取的数据仅供个人使用或者用于科研（机器学习、大数据）一般并无大碍；但如果数据抓取用于商业范畴，就要就事论事了，有可能属于违法也有可能并无大碍。

**如何学好爬虫？**

一句话：**多写代码，从小项目做起，别一个劲啃教程**。

我们平时都说Python爬虫，其实这里可能有个误解，爬虫并不是Python独有的，可以做爬虫的语言有很多例如：PHP,JAVA,C#,C++,Python，选择Python做爬虫是因为下面几点

- 开发效率高，代码简洁，一行代码就可完成请求，100行可以完成一个复杂的爬虫任务；
- 爬虫对于代码执行效率要求不高，网站IO才是最影响爬虫效率的。如一个网页请求可能需要100ms，数据处理10ms还是1ms影响不大；
- 非常多优秀的第三方库，如requests，beautifulsoup，selenium等等；

### 0. 正则表达式

正则表达式是一种查找以及字符串替换操作。比如，我想要在编辑器里匹配所有的数字，怎么办？这就需要正则表达式了。

几乎所有的高级编程语言和代码编辑器都支持正则表达式。但是这玩意学习难度挺高。。。这里先不做介绍，留为回家作业自学（）

请在GitHub上找正则表达式（Regex）的教程，或者玩那种闯关模式的教学网站，比如<https://regexlearn.com>

一些小工具：

- [Test RegEx](https://regexr-cn.com/)
- [I Hate Regex](https://ihateregex.io/)
- [常用正则表达式](https://stackoverflow.org.cn/regexdso/)
- [正则表达式可视化工具](https://c.runoob.com/front-end/7625)

|匹配|正则表达式|
|:-:|:-:|
|网址（URL）|`[a-zA-z]+://[^\s]*`|
|Email地址|`\w+([-+.]\w+)*@\w+([-.]\w+)*\.\w+([-.]\w+)*`|
|QQ号码|`[1-9]\d{4,}`|
|HTML标记(包含内容或自闭合)|`<(.*)(.*)>.*<\/\1>\|<(.*) \/>`|
密码(由数字/大写字母/小写字母/标点符号组成，四种都必有，8位以上)|`(?=^.{8,}$)(?=.*\d)(?=.*\W+)(?=.*[A-Z])(?=.*[a-z])(?!.*\n).*$`|
|中文及全角标点符号(字符)|`[\u3000-\u301e\ufe10-\ufe19\ufe30-\ufe44\ufe50-\ufe6b\uff01-\uffee]`|
|正整数|`[0-9]*[1-9][0-9]*`|

### 1. HTTP请求

要搞明白如何爬虫，先得知道什么是http请求。

> 拓展阅读：[从输入URL到页面加载的过程？如何由一道题完善自己的前端知识体系！](http://www.dailichun.com/2018/03/12/whenyouenteraurl.html)

当你在浏览器中输入一个网址或点击一个链接时，你实际上发起了一个HTTP请求。HTTP请求是你向服务器发送的一种请求，你希望服务器返回特定的信息。

HTTP请求通常由以下几个部分组成：

1. **HTTP方法**：它告诉服务器你希望执行的操作。常见的方法有GET（获取资源）、POST（提交数据）、PUT（更新资源）和DELETE（删除资源），好吧后面两种还是有点不太常见。。。
2. **URL（统一资源定位符）**：它指定了你想要请求的资源的地址。例如，http://www.example.com/index.html。
3. **HTTP标头**：它包含了一些额外的信息，用于描述请求的细节。下面会详细解释。

HTTP响应通常由以下几个部分组成：

1. **状态码**：它告诉你请求的结果是成功还是失败。常见的状态码有200（成功）、404（未找到）和500（服务器错误）等，你经常能看到404 not found，但是其实还有502 badgateway, 403 forbidden等等……其实都是服务器告诉我们浏览器的。
2. **HTTP标头**：它包含了一些额外的信息，用于描述响应的细节。下面会详细解释。
3. **响应正文**：它是服务器返回的实际内容，可以是HTML页面、JSON数据等。

举个例子，你正要扫码抢一个志愿活动。扫码后，其实二维码解密出来就是一个URL，然后填报的页面加载出来了，其实就是你的**微信对这个URL发起了GET请求**。然后问卷星的服务器返回了这份问卷，然后你填写完了，点击「提交」的时候，**微信又对URL发起了POST请求**，这个POST里面当然携带了你填写的所有信息，然后问卷星的服务器上就获得了这些信息，返回一个响应（填写成功的页面，header里面状态码200），顺便给你一个刮刮乐。

> 打开浏览器，按了`F12`进入开发者模式，打开或者刷新网页，切换到network(网络)就可以看到请求和响应的信息了。

HTTP标头（HTTP headers）是HTTP请求和响应中的元数据（metadata），它们提供了关于请求或响应的附加信息。HTTP标头包含了键值对，每个键值对都有一个标头名称和对应的值。

常见的HTTP标头包括：

- **User-Agent**：标识发送请求的用户代理（通常是浏览器）。
- **Content-Type**：指定请求或响应正文的媒体类型，例如text/html或application/json。
- **Content-Length**：指定请求或响应正文的长度。
- **Cookie**：用于在客户端和服务器之间传递状态信息。重点说一下这个cookie哈，它不是曲奇饼干，而是**一段存储在用户计算机上的小型文本**，往往只有几Kb，所以才塞得到这个header里面。那几kb够干啥的？比如说，你登录了一个网站，你也不想刷新以后你的登录状态就丢失了吧？那么cookie就是帮助你保存登录状态的。比如服务器在发送我登录成功的信息时，还在响应头里放入了一段cookie（独一无二的），那浏览器就可以保存它，下次请求的时候就带上它，那么服务器就能认得，原来这就是刚才那家伙，已经登录了。那脑洞大开一下，如果你在请求的时候，用别人登录以后的cookie会怎么样？那当然是你冒名顶替了别人啊，你可以带着这串cookie，获得对他账户的访问权限，相当于“盗号”了。
- **Cache-Control**：指定请求或响应的缓存行为。
- **Authorization**：用于在请求中发送身份验证凭据。

总结起来，HTTP请求是你向服务器发送的请求，希望获取特定的信息或执行特定的操作。HTTP响应是服务器对你的请求的回应，包含了请求的结果或所需的资源。HTTP标头是包含附加信息的键值对（你当然可以用JSON或者字典来描述）。

当然，响应不止有html网页一种哈，这里是对www.fyscu.com发起请求的话，会返回一个网页。那我们用python的话会发生什么呢？

### 2. JSON格式

想一想，如果我想给服务器发送信息，这个信息应该是什么格式的？还是举上面提交问卷的例子，这个信息看起来应该是这样：

```text
姓名：小明
学号：2022123456789
性别：男
```

唉？key-value的形式，这不就是python字典吗？那我们用字典把它写出来：

```json
{
    "姓名": "小明",
    "学号": 2022123456789,
    "性别": "男"
}
```

这其实就是JSON格式，但是和python的字典有些许不同，比如JSON里面`true`不大写。

**基本上所有JSON都可以转变成字典**。传说当年有JSON和XML之争，最后JSON胜在简洁、轻量、易读、能够方便地转化成字典（便于计算机读取），于是JSON成了现在最流行的数据交换格式。

### 3. HTML是什么

比如我们输入了 <https://www.fyscu.com>，向飞扬官网的服务器发送了一个请求(request)，那么服务器就会给我们一个响应(response)，这个响应就是一个HTML页面，如果你点击「查看网页源代码」，就能看到HTML大概长这样：


```html
<html>
<head>
    <title>Title</title>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="http://www.xxx.com/xxx.css">
    <script src="http://www.xxx.com/xxx.js">
    </script>
</head>
<body>
    <p class="title">
        <b>bold text</b>
        normal text
    </p>
</body>
</html>
```

> html标签由很多个节点(Tag、Node)组成。比如 head 、 body 、 p 、 b 。这些节点之间的关系有父子关系、兄弟关系。
父子关系：子节点被包括在父节点中。比如body内的所有标签，都是body的子节点。
兄弟关系：两个节点位于同一层级，比如我们的所有的p标签。他们的直接父节点都是body。

所以，节点之间就像树一样（DOM树），再复杂的页面也是标签套标签形成的。每个标签都可以有自己的一些属性，比如：class、href，src，id。这些属性决定了他们的特点。

### 4. 爬虫快速入门

首先安装requests库，它就是某些大神给我们写好的爬虫框架，python这么适合爬虫很大程度就是因为这些框架给我们省了很多精力，省了很多代码。说实话，不用这些框架我也写不来爬虫。。。

我们需要安装三个模块，后面会介绍它们的用途：

```bash
pip install requests beautifulsoup4 lxml
```

安装好以后，运行下面这个程序，来爬取微博热搜：

```python
import requests
response = requests.get('https://s.weibo.com/top/summary?cate=realtimehot')
print(response.text)
print(response.status_code)  # 200
```

`requests.get()`函数就对微博热搜的URL进行了一次get请求，注意到我们这里没有携带任何的headers和cookies，`response.text`就是一个字符串，整个网页的内容。真的有这么简单吗？我们来看看输出了什么：

```html
<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="Content-type" content="text/html; charset=gb2312"/>
    <title>Sina Visitor System</title>
</head>
<body>
<span id="message"></span>
<script type="text/javascript" src="/js/visitor/mini_original.js?v=20161116"></script>
<script type="text/javascript">
    一堆JS函数
</script>
</body>
</html>
```

怎么一个热搜都没有？这合理吗？为什么我用浏览器就可以？

其实，你第一次用浏览器访问这个页面的时候，服务器发现你没有cookie，所以也返回给浏览器这个东西，**但是这里面的JS代码让浏览器以游客的身份登录了微博**，并获得了一个cookie，作为身份的一个标识。浏览器有cookie了，下次访问的时候就畅通无阻了。

我们用python爬取了这个url以后，当然没法运行JS代码，那怎么办？唉，就像我之前提到的，我们可以「窃取」浏览器的cookie啊，那我们不就处于游客的登录状态了吗？

于是把cookies加到请求中去：（如何获取浏览器的cookie马上就说）

```python
import requests

cookies = {
    'SUB': '_2AkMSD9vLf8NxqwFRmfoUymnraIhywwvEieKkUyoQJRMxHRl-yj9kqkJYtRB6OY_1JJbLkxq_V-5FsAoYEr7yOzTco-_r',
    'SUBP': '0033WrSXqPxfM72-Ws9jqgMF55529P9D9W5JoYoKxZJ1WiP3cS6-NBbL',
}

response = requests.get(
    'https://s.weibo.com/top/summary?cate=realtimehot', cookies=cookies)
print(response.text)
print(response.status_code)  # 200
```

就可以看到微博热搜了是吧，当然这是网页的源代码，看起来比较丑是正常现象。。。

那如何在这么长的字符串中匹配到这些微博热搜呢？曾经，你**只能用正则表达式匹配**，写起来异常繁琐，可读性为0。但是现在有了beautifulsoup4库，敲代码的幸福感直线上升。不过很多教程都把bs4的门槛拉的很高，这不应该啊，人家本来发明出来就是为了简化爬虫的难度的。。。

我的经验是，过一遍前端知识再来看这个，会好很多。

我们先讲一下基本的前端知识。其实，定位/匹配一个或者一堆网页元素，也是CSS解决的痛点之一。那CSS是如何解决的呢？

用**CSS选择器**。不是网页的每个元素都有一些attr（属性）吗，其中有两个很特殊：
- id
- class

```html
<p id="text_id" class="big bold">一段文字</p>
```

比如，匹配这个元素的，可以是：
- `p` 标签选择器
- `#text_id` ID选择器
- `.big` 类选择器（选择了一个类）
- `.bold` 类选择器（选择了一个类）
- `.big.bold` 类选择器（同时选择两个类）

**后代选择器**的语法是用空格分隔的多个选择器组合，它的作用是在 A 选择器的后代元素中找到 B 选择器所指的元素。

```html
<div class="page">
    <div class="article">
        <h1>我是标题部分</h1>
        <p>黑化肥发灰，灰化肥发黑</p>
        <p>黑化肥发灰会挥发；灰化肥挥发会发黑</p>
        <p>黑化肥挥发发灰会花飞；灰化肥挥发发黑会飞花</p>
    </div>
    <p class="footer">版权 © www.luobo.com</p>
</div>
```

`.page p`：表示在.page选定的区域里去找p标签，就找到了所有p标签。假如我们只想查找 class=“article” 元素里的 p 标签呢，就再套一层：`.page .article p`。那如果我只想选择儿子，不要这些子子孙孙呢？

**子元素选择器**：`.page > p` 就行了。

讲了这么多，就是为了给这段代码做铺垫：

```python
# 接上面的代码
from bs4 import BeautifulSoup

soup = BeautifulSoup(response.text, "lxml")
print(soup.select(".td-02 a"))
```

解释一下：
- BeautifulSoup就是从bs4（刚刚下载的python库）中的一个解析函数
- lxml（也是刚刚下的）就相当于告诉BeautifulSoup如何解析：按照html的语法解析。
- 解析完了返回一个特殊的格式，存到soup里面，`soup.select()` 就是选择器语法，我选择了所有含有`class="td-02"`的函数，又继续选择了所有`a`标签。

输出结果是一个List，里面都是我们想要的链接、热搜标题啥的：

```text
[<a href="/weibo?q=%23%E6%97%B6%E6%97%B6%E6%94%BE%E5%BF%83%E4%B8%8D%E4%B8%8B%E7%9A%84%E7%89%B5%E6%8C%82%23&amp;Refer=new_time" target="_blank">时时放心不下的牵挂</a>,
...,
...,
<a href="/weibo?q=%23LPL%E9%80%89%E6%89%8B%E7%BA%A2%E6%AF%AF%E8%A5%BF%E8%A3%85%E6%9D%80%23&amp;t=31&amp;band_rank=50&amp;Refer=top" target="_blank">LPL选手红毯西装杀</a>]
```

那`td-02`，`a`这些是如何找到的呢？那就是网页调试模式(大部分浏览器都是F12)的功劳了。

![80f0ea9e9409b32f649c7427aa2abb30.png](:/3d3f9bfe9c1942f0978b75b45da23318)

### 5. 学会使用F12(审查元素)

1. 定位元素，复制css选择器

![79151112228e01f663f079bfc948447c.png](:/847c947098e944ebbc49253f3b631c90)

可以看到我们想要的元素，有个共同特点就是`class="td-02"`是吧？然后a标签在它里面，自然而然就想到了这个选择器：

`.td-02 a`，完美取出所有匹配的元素！

![eb1eb7bfb2cef3a7b0c4380585bddc86.png](:/cd0a6dae5e1344bb9b8ce936440d3b75)

浏览器也可以自动帮你生成CSS选择器，不过它给的有点蠢。。。只能选这一个，不能匹配到所有，所以仅供参考吧。

2. 网络与资源嗅探

![bc7f95fa1e16cc991dc299d3f07fd5aa.png](:/4c26c806ffd4407cb79ea57c068e2438)

3. 复制curl

![0749ae4242cbd83323e47e0db90e203e.png](:/48bc48b5487041b38acf84c1f4819144)

![9a8c80908a6dc95e04703d51b345b028.png](:/816e767f23c945dca84a2e720029086b)

复制curl很有用，因为一个curl就里面包含了你的header、cookie这些，是一个完整的http请求，而用request直接请求是没有这些的。

> 在Linux课上应该会学curl命令，命令行里常用的请求工具，但是由于过于丑陋，所以诞生了一些替代品，这里不多说。主要是看看这个小工具：<https://www.curlconverter.com>

获取了curl以后把它粘贴到这个网站里面，就能获得完全一样效果的python代码：

![d5f3d372a77a54bbfd65d3b8880d8474.png](:/75022fc4091e426e8f95ba5016251791)

当然这个代码是能跑的，这里的headers看起来很复杂，其实可以把它都删掉，微博不会管的，重要的是cookie。用了这个cookie我们就不是未登录状态了，相当于借用了浏览器的游客身份。

想要使用cookie，这两种方法都可以：

```python
headers = {
    'Cookie': 'SUB=_2AkMSD9vLf8NxqwFRmfoUymnraIhywwvEieKkUyoQJRMxHRl-yj9kqkJYtRB6OY_1JJbLkxq_V-5FsAoYEr7yOzTco-_r; SUBP=0033WrSXqPxfM72-Ws9jqgMF55529P9D9W5JoYoKxZJ1WiP3cS6-NBbL',
}

response = requests.get('https://s.weibo.com/top/summary?cate=realtimehot', headers=headers)
```

or:

```python
cookies = {
    'SUB': '_2AkMSD9vLf8NxqwFRmfoUymnraIhywwvEieKkUyoQJRMxHRl-yj9kqkJYtRB6OY_1JJbLkxq_V-5FsAoYEr7yOzTco-_r',
    'SUBP': '0033WrSXqPxfM72-Ws9jqgMF55529P9D9W5JoYoKxZJ1WiP3cS6-NBbL',
}

response = requests.get(
    'https://s.weibo.com/top/summary?cate=realtimehot', cookies=cookies)
```

（毕竟cookie本身就是header的一部分）这就是我刚刚获得浏览器cookie的渠道。

### 6. 处理数据

观察其中一个a标签：

```html
<a href="/weibo?q=%23%E6%97%B6%E6%97%B6%E6%94%BE%E5%BF%83%E4%B8%8D%E4%B8%8B%E7%9A%84%E7%89%B5%E6%8C%82%23&amp;Refer=new_time" target="_blank">时时放心不下的牵挂</a>
```

发现链接在href属性里面，然后热搜标题在`<a></a>`中间。beautifulsoup返回的soup已经给我们提供了很多方法来获取这些数据，比如：

```python
>>> soup = BeautifulSoup('<a href="/weibo?q=%23%E6%97%B6%E6%97%B6%E6%94%BE%E5%BF%83%E4%B8%8D%E4%B8%8B%E7%9A%84%E7%89%B5%E6%8C%82%23&amp;Refer=new_time" target="_blank">时时放心不下的牵挂</a>', "lxml")
>>> print(soup.prettify())
<html>
 <body>
  <a href="/weibo?q=%23%E6%97%B6%E6%97%B6%E6%94%BE%E5%BF%83%E4%B8%8D%E4%B8%8B%E7%9A%84%E7%89%B5%E6%8C%82%23&amp;Refer=new_time" target="_blank">
   时时放心不下的牵挂
  </a>
 </body>
</html>
>>> tag = soup.a
>>> tag
<a href="/weibo?q=%23%E6%97%B6%E6%97%B6%E6%94%BE%E5%BF%83%E4%B8%8D%E4%B8%8B%E7%9A%84%E7%89%B5%E6%8C%82%23&amp;Refer=new_time" target="_blank">时时放心不下的牵挂</a>
>>> type(tag)
<class 'bs4.element.Tag'>
>>> tag.string
'时时放心不下的牵挂'
>>> tag.attrs
{'href': '/weibo?q=%23%E6%97%B6%E6%97%B6%E6%94%BE%E5%BF%83%E4%B8%8D%E4%B8%8B%E7%9A%84%E7%89%B5%E6%8C%82%23&Refer=new_time', 'target': '_blank'}
>>> tag.attrs['href']
'/weibo?q=%23%E6%97%B6%E6%97%B6%E6%94%BE%E5%BF%83%E4%B8%8D%E4%B8%8B%E7%9A%84%E7%89%B5%E6%8C%82%23&Refer=new_time'
>>>
```

这里需要注意的是，soup里面包含body、html节点，也就是说对于不标准的HTML字符串，BeautifulSoup可以自动更正格式（lxml解析器带来的好处）。
- `tag.string`：是这个标签里面的所有文本。
- `tag.attrs`：是一个字典，一个标签所有的属性。
- `tag.a`：在tag里面找到的第一个a标签。

```python
list_of_all_tags = soup.select(".td-02 a")

for tag in list_of_all_tags:
    text = tag.string
    href = "https://s.weibo.com" + tag.attrs["href"]
    result = "- [{}]({})".format(text, href)
    print(result)
```

返回的是我们喜闻乐见的markdown格式链接：

```markdown
- [习主席旧金山之行](https://s.weibo.com/weibo?q=%23%E4%B9%A0%E4%B8%BB%E5%B8%AD%E6%97%A7%E9%87%91%E5%B1%B1%E4%B9%8B%E8%A1%8C%23&Refer=new_time)
- [中美元首会晤](https://s.weibo.com/weibo?q=%23%E4%B8%AD%E7%BE%8E%E5%85%83%E9%A6%96%E4%BC%9A%E6%99%A4%23&t=31&band_rank=1&Refer=top)
- [孙颖莎出现时全场的反应](https://s.weibo.com/weibo?q=%E5%AD%99%E9%A2%96%E8%8E%8E%E5%87%BA%E7%8E%B0%E6%97%B6%E5%85%A8%E5%9C%BA%E7%9A%84%E5%8F%8D%E5%BA%94&t=31&band_rank=2&Refer=top)
- [纯享版歼10C飞行表演大片](https://s.weibo.com/weibo?q=%23%E7%BA%AF%E4%BA%AB%E7%89%88%E6%AD%BC10C%E9%A3%9E%E8%A1%8C%E8%A1%A8%E6%BC%94%E5%A4%A7%E7%89%87%23&t=31&band_rank=3&Refer=top)
- [孙颖莎虽迟但到](https://s.weibo.com/weibo?q=%23%E5%AD%99%E9%A2%96%E8%8E%8E%E8%99%BD%E8%BF%9F%E4%BD%86%E5%88%B0%23&t=31&band_rank=4&Refer=top)
- [竞燃之夜](https://s.weibo.com/weibo?q=%E7%AB%9E%E7%87%83%E4%B9%8B%E5%A4%9C&t=31&band_rank=5&Refer=top)
```

### 7. 爬虫伪装

网站可以通过IP和header确认你的身份。一旦发现你这个人不停地访问我的网站，什么100毫秒就刷新一次，是人能做出来的操作吗。。所以完全可以把你这个IP封掉。但是这种反爬虫不是所有网站都有的（应该说大部分网站都没有，看看我们的12306都被逼成啥样了）所谓道高一尺，魔高一丈，于是IP池、伪造请求头这些东西就诞生了。

IP池的目的就是允许我都用不同的IP轮流去请求，服务器就会认为这个爬虫是很多人，就不会封我。再说了，封了一个IP，我不是还有一堆吗？当然，IP池很贵，一般的爬虫都不会用这个。

伪造请求头这个更容易一些，目的就是模仿不同的人的请求（请求头里有你的浏览器、Referer等关键信息，每次都用一个请求头，请求几千次，就容易露馅）

```
pip install my-fake-useragent
```

```python
from my_fake_useragent import UserAgent
import requests

ua = UserAgent(family='chrome')
res = ua.random()
url="https://www.baidu.com"
headers={"User-Agent":res}
response=requests.get(url=url,headers=headers)
print(response.status_code)  # 打印状态码
print(response.request.headers)  # 打印自己的请求头
```

### 8. Helium

requests请求的时候，有几个问题。第一，很多内容需要执行js才能显现，像刚才的微博热搜一样，但用cookie能够勉强解决。第二，**如果这个网页是动态的呢**？

举个很常见的例子，当我们在浏览网站时，随着不断向下滑动网页，当前页面会不断刷新出新的内容，但浏览器址栏上的URL却始终没有变化。这种由JavaScript动态生成的页面，当我们通过浏览器查看它的网页源代码时，往往找不到页面上显示的内容。

抓取动态页面有两种常用的方法：

- 通过JavaScript逆向工程获取动态数据接口（真实的访问路径）
- **模拟真实浏览器**，获取JavaScript渲染后的内容。

第一种方法比较考验技巧，这里不多做介绍，重点讲一下第二种方法：即与requests平起平坐的Selenium库。使用selenium库的时候，会直接打开一个浏览器窗口（需要下载Webdriver），渲染出你能看到的任何页面。python脚本干的事就相当于进行了**网页自动化**操作，selenium就是一个**浏览器自动化框架**。

但是虽说selenium能干的事包含了requests，但是仍然无法替代它，因为：

- **速度慢**。每次运行爬虫都打开一个浏览器，如果没有设置，还会加载图片、JS等等一大堆东西；占用资源太多。
- **对网络的要求会更高**。Selenium 加载了很多可能对你没有价值的补充文件（如css，js和图像文件）与仅仅用requests请求真正需要的资源相比，这可能会产生更多的流量。
- **爬取规模不能太大**：是由上面两点决定的。如果像百度爬虫一样要爬几亿条数据的话，浏览器绝对是胜任不了的。

我们不着急装Selenium，先来看看Helium：一个高度封装了Selenium的爬虫库（所以它全称Selenium-Python-Helium），非常非常简单，适合入门。

而且helium不需要像selenium一样，爬个虫还要下一个浏览器（webdriver），helium自带的webdriver可以配合系统里的浏览器直接运行。由于轻量而且容易上手，所以取名为He（氦），可比Se（硒）轻了不止一点啊。

![](https://github.com/mherrmann/helium/raw/master/docs/helium-demo.gif)

但是教程比Selenium少多了，官方文档在这里：[Helium Cheatsheet](https://github.com/mherrmann/helium/blob/master/docs/cheatsheet.md)

<!-- 顺便提一嘴，用一个库代替另一个库是很平常的事情，我当年学的是bs4，但是现在xpath都快把bs4取代了，学习成本更低，更强大。但是我还没学，教不了你们…… 有兴趣真得去学一学。 -->

### 9. 保存数据到本地

### 10. 分析数据

### 11. 数据可视化
